# Projet AI Prediction - Classification du Cancer du C√¥lon

## üî¨ Description
Ce projet applique des algorithmes de classification supervis√©e sur un jeu de donn√©es d'expression g√©nique du cancer du c√¥lon (provenant de Kaggle) pour classer les √©chantillons en deux cat√©gories : **"normal"** et **"cancer"**. L'objectif est de d√©velopper un mod√®le capable d'automatiser la d√©tection de cette maladie √† partir des profils d'expression g√©nique.

---

## üîß Pr√©requis
- Python 3.x
- Jupyter Notebook
- Biblioth√®ques : `pandas`, `numpy`, `matplotlib`, `seaborn`, `sklearn`

Installez les d√©pendances avec :
```bash
pip install pandas numpy matplotlib seaborn scikit-learn
```

---

## üîç Installation
1. Clonez le r√©f√©rentiel :
```bash
git clone https://github.com/ihssane2002/Projet-AI_prediction.git
```
2. Acc√©dez au projet :
```bash
cd Projet-AI_prediction
```
3. Lancez le notebook :
```bash
jupyter notebook Projet_AI_Prediction.ipynb
```

---

## üìâ Utilisation
1. **Pr√©paration des donn√©es** : Nettoyage, standardisation, et division des donn√©es en ensembles d'entra√Ænement et de test.
2. **Exploration** : Analyse de la distribution des classes et visualisation des donn√©es (histogrammes, boxplots, pairplots).
3. **Mod√©lisation** : Entra√Ænement de plusieurs algorithmes de classification :
   - R√©gression Logistique
   - Support Vector Machine (SVM)
   - k-Nearest Neighbors (k-NN)
   - Arbre de D√©cision
   - For√™t Al√©atoire
4. **Evaluation** : Utilisation de la matrice de confusion, du rapport de classification, et de la pr√©cision globale pour comparer les mod√®les.
5. **Interpr√©tation** : Identification des g√®nes les plus influents pour la classification.

---

## üìä Mod√®les Utilis√©s
- **R√©gression Logistique** : Simple et efficace pour des donn√©es lin√©airement s√©parables.
- **SVM** : Utilisation d'un noyau RBF pour capturer des relations complexes.
- **k-NN** : Classifie les √©chantillons selon leurs plus proches voisins.
- **Arbre de D√©cision** : Facile √† interpr√©ter mais sujet au surapprentissage.
- **For√™t Al√©atoire** : Combine plusieurs arbres pour am√©liorer la robustesse.

---

## üìä R√©sultats
- Les mod√®les ont atteint une pr√©cision de **100%** pour la R√©gression Logistique, SVM, et k-NN.
- Les g√®nes **SLC7A5**, **RNF43**, et **UGP2** se sont r√©v√©l√©s les plus importants dans la classification.
- L'approche multi-mod√®le a permis de valider la robustesse des pr√©dictions.

---

## üë®‚Äçüíº Auteur
- **Ihssane BAMMAD**



